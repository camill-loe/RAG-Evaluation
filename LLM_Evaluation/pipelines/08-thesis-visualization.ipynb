{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91f33d72-ec1d-4e3b-a22e-e1897d831618",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import linregress\n",
    "import json\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import f_oneway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6c34c0-c85d-4cb6-bc9d-0bb7165dc642",
   "metadata": {},
   "source": [
    "### Read in the files and save them as dataframes in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f931fef-c715-4687-83c3-bcf9f6e92e1e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames loaded from parquet files: ['complete_anonymized_results_run2_subset240928203654', 'complete_anonymized_results_run3_subset240928203654', 'complete_anonymized_results_run1_subset240928203654', 'complete_anonymized_results_240926', 'complete_anonymized_results_run5_3subset240928203654', 'complete_anonymized_results_240928203654', 'complete_anonymized_results_run5_subset240928203654', 'complete_anonymized_results_run5_2subset240928203654', 'complete_anonymized_results_240924', 'complete_anonymized_results_240928142914', 'complete_anonymized_results_run4_subset240928203654']\n"
     ]
    }
   ],
   "source": [
    "# reads in the question_answer_pairs parquet and retrieves the highest timestamp\n",
    "#base_folder = \"/Volumes/uc-catalog-dev/advancedanalytics-productai-dev/transformed_dev/llm-evaluation/\" + datetime.now().strftime(\"%Y-%m-%d\") + \"/\"\n",
    "#df = pd.read_parquet(base_folder + \"question_answer_pairs+productai_answers+evaluation_results_20240928203654_subset_run1.parquet\")\n",
    "\n",
    "def read_parquets_to_dict(folder_path):\n",
    "    # Create an empty dictionary to store dataframes\n",
    "    df_dict = {}\n",
    "    \n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.parquet') and filename.startswith(\"complete_anonymized\"):  # Check for parquet files\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # Read the parquet file into a pandas DataFrame\n",
    "            df = pd.read_parquet(file_path)\n",
    "            \n",
    "            # Add the DataFrame to the dictionary with the file name (without extension) as the key\n",
    "            file_key = os.path.splitext(filename)[0]\n",
    "            df_dict[file_key] = df\n",
    "    \n",
    "    return df_dict\n",
    "\n",
    "base_folder = \"/home/jovyan/LLM_evaluation/runs/\"  # Replace with the path to your folder containing parquet files\n",
    "dfs_dict = read_parquets_to_dict(base_folder)\n",
    "\n",
    "# Optional: Check the keys in the dictionary\n",
    "print(\"DataFrames loaded from parquet files:\", list(dfs_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c96b1b-1ffe-4c91-9d91-0b27e9b44833",
   "metadata": {},
   "source": [
    "### Comparison of Product AI responses to the same questions\n",
    "ANOVA = Analysis of Variance \\\n",
    "The F-statistic is the ratio of the variance between the groups to the variance within the groups. \\\n",
    "If the F-statistic is large, it suggests a significant difference between the group means. \\\n",
    "High p-value means insignificant differences. \\\n",
    "One-Way ANOVA: Compares the means of three or more groups based on one independent variable. For example, comparing test scores across different teaching methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38130813-4f65-4c1a-93d9-ac7d198fbe89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA F-statistic: 0.016651206409720593\n",
      "ANOVA p-value: 0.9994555390362249\n"
     ]
    }
   ],
   "source": [
    "productai_runs_keys = ['complete_anonymized_results_run1_subset240928203654', 'complete_anonymized_results_run2_subset240928203654', \n",
    "                       'complete_anonymized_results_run3_subset240928203654', 'complete_anonymized_results_run4_subset240928203654', \n",
    "                       'complete_anonymized_results_run5_subset240928203654']\n",
    "\n",
    "# Create a subset dictionary\n",
    "productai_runs_dict = {key: dfs_dict[key] for key in productai_runs_keys if key in dfs_dict}\n",
    "\n",
    "for df in productai_runs_dict.values():\n",
    "    df.sort_values(by='question', inplace=True)\n",
    "\n",
    "# Extract evaluation scores from each DataFrame\n",
    "scores_list = [df['evaluation_score'].values for df in productai_runs_dict.values()]\n",
    "\n",
    "# Perform ANOVA\n",
    "anova_result = f_oneway(*scores_list)\n",
    "\n",
    "# Output the results\n",
    "print(\"ANOVA F-statistic:\", anova_result.statistic)\n",
    "print(\"ANOVA p-value:\", anova_result.pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6b1670-d4e0-4717-930b-5061126c50f9",
   "metadata": {},
   "source": [
    "### Count how often there was a difference in the evaluation score in one of the five documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c2e84ee-f080-43b5-bed9-00a9845cb9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions with different evaluation scores: 40\n"
     ]
    }
   ],
   "source": [
    "# Merge DataFrames on the 'question' column\n",
    "merged_df = productai_runs_dict['complete_anonymized_results_run1_subset240928203654'][['question']].copy()  # Start with 'question' column from the first DataFrame\n",
    "\n",
    "# Add evaluation scores from each DataFrame to the merged DataFrame\n",
    "for name, df in productai_runs_dict.items():\n",
    "    merged_df[name] = df.sort_values(by='question')['evaluation_score'].values\n",
    "\n",
    "# Check for questions where the evaluation scores are different across DataFrames\n",
    "merged_df['different_scores'] = merged_df.iloc[:, 1:].nunique(axis=1) > 1\n",
    "\n",
    "# Count how many questions have different evaluation scores\n",
    "different_questions_count = merged_df['different_scores'].sum()\n",
    "\n",
    "print(\"Number of questions with different evaluation scores:\", different_questions_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d7e47-64f4-466c-a4ea-81d62dd09060",
   "metadata": {},
   "source": [
    "### Comparison of Evaluation results to the same Product AI responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "604268f9-5df4-47a7-b01f-2f513d443dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA F-statistic: 0.13172600989940925\n",
      "ANOVA p-value: 0.8766323207889749\n"
     ]
    }
   ],
   "source": [
    "evaluation_runs_keys = ['complete_anonymized_results_run5_subset240928203654', 'complete_anonymized_results_run5_2subset240928203654',\n",
    "                        'complete_anonymized_results_run5_3subset240928203654']\n",
    "\n",
    "# Create a subset dictionary\n",
    "evaluation_runs_dict = {key: dfs_dict[key] for key in evaluation_runs_keys if key in dfs_dict}\n",
    "\n",
    "for df in evaluation_runs_dict.values():\n",
    "    df.sort_values(by='question', inplace=True)\n",
    "\n",
    "# Extract evaluation scores from each DataFrame\n",
    "scores_list = [df['evaluation_score'].values for df in evaluation_runs_dict.values()]\n",
    "\n",
    "# Perform ANOVA\n",
    "anova_result = f_oneway(*scores_list)\n",
    "\n",
    "# Output the results\n",
    "print(\"ANOVA F-statistic:\", anova_result.statistic)\n",
    "print(\"ANOVA p-value:\", anova_result.pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28c2576-b49f-45a8-b8ac-e58f0c97f118",
   "metadata": {},
   "source": [
    "### Count how often there was a difference in the evaluation score in one of the three documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c305b48-c5ef-4a1f-a208-a9188f2b14dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions with different evaluation scores: 25\n"
     ]
    }
   ],
   "source": [
    "# Merge DataFrames on the 'question' column\n",
    "merged_df = evaluation_runs_dict['complete_anonymized_results_run5_subset240928203654'][['question']].copy()  # Start with 'question' column from the first DataFrame\n",
    "\n",
    "# Add evaluation scores from each DataFrame to the merged DataFrame\n",
    "for name, df in evaluation_runs_dict.items():\n",
    "    merged_df[name] = df.sort_values(by='question')['evaluation_score'].values\n",
    "\n",
    "# Check for questions where the evaluation scores are different across DataFrames\n",
    "merged_df['different_scores'] = merged_df.iloc[:, 1:].nunique(axis=1) > 1\n",
    "\n",
    "# Count how many questions have different evaluation scores\n",
    "different_questions_count = merged_df['different_scores'].sum()\n",
    "\n",
    "print(\"Number of questions with different evaluation scores:\", different_questions_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
