{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a54f7d-2aec-429b-92be-53e91e4a021e",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "This is a notebook that should read in a dataframe and calculates how much it costed in € to prompt the LLMs for the generation of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5416a4f-1d58-4244-b394-ebb6c94c8e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m698.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.11/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
      "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.8/792.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, tiktoken\n",
      "Successfully installed regex-2024.9.11 tiktoken-0.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "614e8447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import json\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02006f37-a1a4-4d4f-a13d-fad53e23847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pair_prompt = \"\"\"\n",
    "        You are an expert in the Validation Services division of a pharmaceutical company. You hold a PhD in medicine, pharmacy, and biochemistry and possess extensive knowledge of regulatory compliance and quality assurance in the pharmaceutical industry. You are adept at analyzing and interpreting regulatory documents, extractable guides, and scientific research papers in biotech, biology, and pharmacy. Your expertise spans regulatory compliance, quality assurance, and scientific research methodologies in the pharmaceutical industry. You work at a company that provides products and services for drug development, biotechnology, and life science research, including laboratory instruments, consumables, and process technologies. \n",
    "        Instructions:\n",
    "        You are given a text extract and should generate 2 different questions from that text, which relate to a product in the text. The questions don't have to be related to the same product. You should also generate the answers to the questions.\n",
    "        Your answer is always of exactly the format: \"[{'question': 'the question', 'answer': 'the answer'}, {'question': 'the question', 'answer': 'the answer'},]\"\n",
    "        \"\"\"\n",
    "evaluator_prompt = \"\"\"\n",
    "        You are tasked with evaluating the quality of answers generated by a Retrieval-Augmented Generation (RAG)-based chatbot designed to answer product-related questions in the validation service department of a pharmaceutical company. The evaluation involves comparing the generated answers against the true answers provided by experts.\n",
    "        Your goal is to evaluate how well the generated answers match the true answers based on the following criteria:\n",
    "        - Accuracy: How closely does the generated answer align with the true answer in terms of factual correctness?\n",
    "        - Completeness: Does the generated answer fully address the question, or does it omit important details?\n",
    "        - Clarity: Is the generated answer clear and understandable, or is it ambiguous or confusing?\n",
    "        - Relevance: Is the generated answer directly relevant to the question asked, or does it include unnecessary or off-topic information?\n",
    "        Instructions:\n",
    "        Compare the true answer and the generated answer using the criteria above.\n",
    "        Start by writing an Evaluation Text that provides a detailed comparison, explaining why the generated answer received a particular score. Your reasoning should highlight the strengths and weaknesses of the generated answer in relation to the true answer.\n",
    "        Based on the comparison, assign an Evaluation Score on a scale from 1 to 5:\n",
    "        5: The generated answer is nearly identical to the true answer in accuracy, completeness, clarity, and relevance.\n",
    "        4: The generated answer is very close to the true answer but may miss some minor details or contain slight inaccuracies.\n",
    "        3: The generated answer provides a reasonably correct response but has noticeable gaps in completeness, clarity, or relevance.\n",
    "        2: The generated answer contains significant inaccuracies or omissions but has some elements that are correct or relevant.\n",
    "        1: The generated answer is mostly incorrect or irrelevant.\n",
    "        Output Example:\n",
    "        At the end of the evaluation, return the result as a JSON object with the following structure:\n",
    "        {\n",
    "        \"reasoning\": \"Your detailed comparison goes here, explaining why the generated answer received its score based on accuracy, completeness, clarity, and relevance.\",\n",
    "        \"score\": \"Your score from 1 to 5 goes here.\"\n",
    "        }\n",
    "        Here are the question and answers for evaluation: \n",
    "            Question: , \n",
    "            True answer: , \n",
    "            Generated answer: .\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c1c4805-6b40-4c3c-8360-29d9f339ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pricing from https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/\n",
    "azure_openai_api_key = \"example\"#dbutils.secrets.get(scope='keyvault-link', key='azure-openai-api-key')\n",
    "MODELS = {\n",
    "    'MODEL_GPT4o': {\n",
    "        \"endpoint\": \"https://appprodsagopenaigpt4weu.openai.azure.com/openai/deployments/gpt-4o-cad-extraction/chat/completions?api-version=2024-02-15-preview\",\n",
    "        \"api_key\": azure_openai_api_key,\n",
    "        \"prompt_token_cost\": 0.0047/1000,\n",
    "        \"completion_token_cost\": 0.0139/1000\n",
    "    },\n",
    "    'MODEL_GPT4o_mini': {\n",
    "        \"endpoint\": \"https://appprodsagopenaigpt4weu.openai.azure.com/openai/deployments/evaluation_gpt4o-mini/chat/completions?api-version=2023-03-15-preview\",\n",
    "        \"api_key\": azure_openai_api_key,\n",
    "        \"prompt_token_cost\": 0.00014/1000,\n",
    "        \"completion_token_cost\": 0.0006/1000\n",
    "    }\n",
    "    # Add more endpoints and api keys here\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c934cae-1f97-4e65-bdeb-d4a30ffd431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_costs(token_counts:dict, model:str='MODEL_GPT4o'):\n",
    "    completion_tokens = token_counts['completion_tokens']\n",
    "    prompt_tokens = token_counts['prompt_tokens']\n",
    "    costs_in_euro = prompt_tokens * MODELS[model][\"prompt_token_cost\"] + \\\n",
    "            completion_tokens * MODELS[model][\"completion_token_cost\"]\n",
    "    return round(costs_in_euro, 4)\n",
    "def count_tokens(text):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    return len(encoding.encode(text))\n",
    "def transform_dataframe(df):\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].astype(str).apply(count_tokens)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fd5808-ad45-4beb-8d58-0879220e42e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the question_answer_pairs parquet and retrieves the highest timestamp\n",
    "base_folder = \"/Volumes/uc-catalog-dev/advancedanalytics-productai-dev/transformed_dev/llm-evaluation/\" + datetime.now().strftime(\"%Y-%m-%d\") + \"/\"\n",
    "base_folder = \"/Volumes/uc-catalog-dev/advancedanalytics-productai-dev/transformed_dev/llm-evaluation/2024-09-28/\"\n",
    "combined_df = pd.read_parquet(base_folder + f\"question_answer_pairs+productai_answers+evaluation_results.parquet\")\n",
    "display(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebf88dd-d57b-4f45-96d0-78a51a7b1926",
   "metadata": {},
   "source": [
    "#### Calculation variables\n",
    "\n",
    "##### Generating QA Pairs\n",
    "- Model = GPT-4o-mini\n",
    "- Input = (Prompt + Chunk size) * number of chunks\n",
    "- Output = Questions + Answers\n",
    "\n",
    "##### Generating Product AI response\n",
    "- Model = GPT-4o\n",
    "- Input = Questions\n",
    "- Output = Product AI response\n",
    "\n",
    "##### Evaluating answer\n",
    "- Model = GPT-4o-mini\n",
    "- Input = (Prompt + Question + Answer + Product AI response) * number of questions\n",
    "- Output = (Score + Reasoning) * number of questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec470eab-163a-4d5d-b267-bf1a955dd874",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df = transform_dataframe(combined_df)\n",
    "display(token_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc61d678-2c31-44f8-800d-e6f247c68431",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.read_parquet(\"/home/jovyan/LLM_evaluation/runs/complete_anonymized_results_240928142914.parquet\")\n",
    "token_df = pd.read_parquet(\"/home/jovyan/LLM_evaluation/runs/tokenized_results_240928142914.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cb56dc2-bb4e-45cf-9f1d-a8a87454fea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_tokens': 1784504, 'completion_tokens': 32828, 'total_tokens': 1817332}\n"
     ]
    }
   ],
   "source": [
    "input_tokens_qa_pairs = count_tokens(qa_pair_prompt) * len(token_df) + token_df['chunk'].sum()\n",
    "output_tokens_qa_pairs = token_df['question'].sum() + token_df['answer'].sum()\n",
    "total_tokens_qa_pairs = input_tokens_qa_pairs + output_tokens_qa_pairs\n",
    "token_counts_qa_pairs = {\n",
    "    'prompt_tokens': input_tokens_qa_pairs,\n",
    "    'completion_tokens': output_tokens_qa_pairs,\n",
    "    'total_tokens': total_tokens_qa_pairs\n",
    "}\n",
    "costs_qa_pairs = calculate_costs(token_counts_qa_pairs, model='MODEL_GPT4o')\n",
    "print(token_counts_qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d87edff-3815-41cd-a76f-1d6bc44b183d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_tokens': 14738, 'completion_tokens': 85926, 'total_tokens': 100664}\n"
     ]
    }
   ],
   "source": [
    "input_tokens_productai = token_df['question'].sum()\n",
    "output_tokens_productai = token_df['productai_response'].sum()\n",
    "total_tokens_productai = input_tokens_productai + output_tokens_productai\n",
    "token_counts_productai = {\n",
    "    'prompt_tokens': input_tokens_productai,\n",
    "    'completion_tokens': output_tokens_productai,\n",
    "    'total_tokens': total_tokens_productai\n",
    "}\n",
    "costs_productai = calculate_costs(token_counts_productai, model='MODEL_GPT4o')\n",
    "print(token_counts_productai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "990bd285-228c-42c9-811d-f2dc1f903c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_tokens': 422612, 'completion_tokens': 87897, 'total_tokens': 510509}\n"
     ]
    }
   ],
   "source": [
    "input_tokens_evaluation = count_tokens(evaluator_prompt) * len(token_df) + token_df['question'].sum() + token_df['answer'].sum() + token_df['productai_response'].sum()\n",
    "output_tokens_evaluation = token_df['evaluation_score'].sum() + token_df['evaluation_reasoning'].sum()\n",
    "total_tokens_evaluation = input_tokens_evaluation + output_tokens_evaluation\n",
    "token_counts_evaluation = {\n",
    "    'prompt_tokens': input_tokens_evaluation,\n",
    "    'completion_tokens': output_tokens_evaluation,\n",
    "    'total_tokens': total_tokens_evaluation\n",
    "}\n",
    "costs_evaluation = calculate_costs(token_counts_evaluation, model='MODEL_GPT4o')\n",
    "print(token_counts_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "402f8937-b319-469c-8bc2-f3ba3d012487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Costs for QA pair generation: 8.8435€. Total tokens used: 1817332'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Costs for ProductAI prompts: 1.2636€. Total tokens used: 100664'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Costs for Evaluation: 3.208€. Total tokens used: 510509'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Total costs: 13.315100000000001€'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Total tokens used: 2428505'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(f\"Costs for QA pair generation: {costs_qa_pairs}€. Total tokens used: {total_tokens_qa_pairs}\")\n",
    "display(f\"Costs for ProductAI prompts: {costs_productai}€. Total tokens used: {total_tokens_productai}\")\n",
    "display(f\"Costs for Evaluation: {costs_evaluation}€. Total tokens used: {total_tokens_evaluation}\")\n",
    "display(f\"Total costs: {costs_qa_pairs + costs_productai + costs_evaluation}€\")\n",
    "display(f\"Total tokens used: {total_tokens_qa_pairs + total_tokens_productai + total_tokens_evaluation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
