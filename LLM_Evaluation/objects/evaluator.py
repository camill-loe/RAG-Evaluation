from datasets import Dataset
import requests
import time
import ast

class Evaluator:
    def __init__(self, question, true_answer, generated_answer, model, api_key, document=None):
        """
        Initializes the Evaluator class.

        Args:
            question (str): The question being evaluated.
            true_answer (str): The true answer provided by experts.
            generated_answer (str): The answer generated by the model.
            model (str): The model used for generating answers.
            api_key (str): The API key for accessing the model.
            document (str, optional): Additional document for context. Defaults to None.
        """
        self.question = question
        self.true_answer = true_answer
        self.generated_answer = generated_answer
        self.document = document
        self.model = model
        self.api_key = api_key

    def _evaluate(self, message, instruction):
        """
        Evaluates the generated answer using the specified model.

        Args:
            message (str): The message containing the question and answers.
            instruction (str): The instruction for evaluating the answer.

        Returns:
            dict: The evaluation result containing the score and reasoning.

        Raises:
            KeyError: If the 'choices' key is missing or empty in the response.
        """
        headers = {"Content-Type": "application/json", 
                   "api-key": self.api_key}
        payload = {
            "messages": [
                {"role": "system", "content": [{"type": "text", "text": message}]},
                {"role": "user", "content": [{"type": "text", "text": instruction+ """
                Your answer is always of exactly the format {"score": "the integer score", "reasoning": "the reasoning"}.
                """}]}
            ],
            "temperature": 0.7, "top_p": 0.95, "max_tokens": 4096
        }
        GPT4V_ENDPOINT = f"https://appprodsagopenaigpt4weu.openai.azure.com/openai/deployments/{self.model}/chat/completions?api-version=2024-02-15-preview"

        while True:
            response = requests.post(GPT4V_ENDPOINT, headers=headers, json=payload)
            if response.status_code == 200:
                response_json = response.json()
                if "choices" in response_json and len(response_json["choices"]) > 0:
                    return ast.literal_eval(response_json["choices"][0]["message"]["content"])
                else:
                    raise KeyError("The 'choices' key is missing or empty in the response.")
            elif response.status_code == 429:
                time.sleep(60)
            else:
                response.raise_for_status()

    def evaluate_correctness(self):
        """
        Evaluates the correctness of the generated answer.

        Returns:
            dict: The evaluation result containing the score and reasoning.
        """
        message = f"""
            Here are the question and answers for evaluation: 
            Question: {self.question}, 
            True answer: {self.true_answer}, 
            Generated answer: {self.generated_answer}.
            """
        instruction = """
            You are tasked with evaluating the quality of answers generated by a Retrieval-Augmented Generation (RAG)-based chatbot designed to answer product-related questions in the validation service department of a pharmaceutical company. The evaluation involves comparing the generated answers against the true answers provided by experts.
            
            Your goal is to evaluate how well the generated answers match the true answers based on the following criteria:
            
            - Accuracy: How closely does the generated answer align with the true answer in terms of factual correctness?
            - Completeness: Does the generated answer fully address the question, or does it omit important details?
            - Clarity: Is the generated answer clear and understandable, or is it ambiguous or confusing?
            - Relevance: Is the generated answer directly relevant to the question asked, or does it include unnecessary or off-topic information?
            
            Instructions:
            
            Compare the true answer and the generated answer using the criteria above.
            Start by writing an Evaluation Text that provides a detailed comparison, explaining why the generated answer received a particular score. Your reasoning should highlight the strengths and weaknesses of the generated answer in relation to the true answer.
            Based on the comparison, assign an Evaluation Score on a scale from 1 to 5:
                5: The generated answer is nearly identical to the true answer in accuracy, completeness, clarity, and relevance.
                4: The generated answer is very close to the true answer but may miss some minor details or contain slight inaccuracies.
                3: The generated answer provides a reasonably correct response but has noticeable gaps in completeness, clarity, or relevance.
                2: The generated answer contains significant inaccuracies or omissions but has some elements that are correct or relevant.
                1: The generated answer is mostly incorrect or irrelevant.
            
            Output Example:
            
            At the end of the evaluation, return the result as a JSON object with the following structure:
            
            {
            "reasoning": "Your detailed comparison goes here, explaining why the generated answer received its score based on accuracy, completeness, clarity, and relevance. It should be a maximum of five sentences.",
            "score": "Your score from 1 to 5 goes here."
            }

            """
        return self._evaluate(message, instruction)

    def evaluate_relevance(self):
        """
        Placeholder method for evaluating the relevance of the generated answer.
        """
        pass

    def evaluate_metrical_correctness(self):
        """
        Evaluates the metrical correctness of the generated answer using predefined metrics.

        Returns:
            pandas.DataFrame: The evaluation scores.
        """
        data_samples = {
            'question': [self.question],
            'answer': [self.generated_answer],
            'ground_truth': [self.true_answer]
        }
        dataset = Dataset.from_dict(data_samples)
        score = evaluate(dataset, metrics=[answer_correctness])
        score.to_pandas()
        return score